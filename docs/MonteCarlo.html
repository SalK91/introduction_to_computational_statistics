
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Introduction to Monte Carlo Methods &#8212; Introduction to Statistical Computing for Aspiring Data Scientists</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Introduction to PyMC3" href="PyMC3.html" />
    <link rel="prev" title="Bayesian vs Frequentist statistics" href="notebooks.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo_large.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Introduction to Statistical Computing for Aspiring Data Scientists</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_by_example.html">
   An Introductory Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="learn_more.html">
   Learn More
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Computational Statistics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks.html">
   Bayesian vs Frequentist statistics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Monte Carlo Methods
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to Monte Carlo Methods
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  PyMC3 for Bayesian Modeling and Inference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="PyMC3.html">
   Introduction to PyMC3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Covid_forecast.html">
   Covid Forecast with PyMC3
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/MonteCarlo.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sjster/statistical_computing_book/master?urlpath=tree/mini_book/docs/MonteCarlo.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sjster/statistical_computing_book/blob/master/mini_book/docs/MonteCarlo.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-blocks">
   Building blocks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-walks">
     Random Walks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-does-this-work">
     Why does this work?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proposal-distribution">
     Proposal distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#foundation-of-bayesian-inference">
     Foundation of Bayesian Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-metropolis-algorithm">
   The Metropolis Algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#outline-of-the-metropolis-algorithm">
     Outline of the Metropolis algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-details">
     The details
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#traceplot">
     Traceplot
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#building-the-inferred-distribution">
     Building the Inferred Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representing-the-inferred-distribution">
     Representing the Inferred Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notes-about-the-metropolis-algorithm">
     Notes about the Metropolis algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#python-code-for-walkthrough-of-algorithm">
     Python Code for walkthrough of algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summarize-the-above-distribution-mean-variance-minimum-and-maximum-quartiles">
     Summarize the above distribution - Mean, Variance, Minimum and Maximum, Quartiles
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-metropolis-hastings-algorithm">
   The Metropolis-Hastings Algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hamiltonian-monte-carlo-also-called-hybrid-monte-carlo">
   Hamiltonian Monte Carlo (also called Hybrid Monte Carlo)
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="introduction-to-monte-carlo-methods">
<h1>Introduction to Monte Carlo Methods<a class="headerlink" href="#introduction-to-monte-carlo-methods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://pubs.er.usgs.gov/publication/70204463">Beginning Bayesian Statistics</a></p>
<p><a class="reference external" href="https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/">Hamiltonian Monte Carlo in Python</a></p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=VnNdhsm0rJQ">Betancourt HMC - Best introduction to HMC</a></p>
<p><a class="reference external" href="http://arxiv.org/abs/1111.4246">NUTS paper</a></p>
<p><a class="reference external" href="https://colcarroll.github.io/hmc_tuning_talk/">HMC Tuning by Colin Caroll</a></p>
</div>
<div class="section" id="building-blocks">
<h2>Building blocks<a class="headerlink" href="#building-blocks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="random-walks">
<h3>Random Walks<a class="headerlink" href="#random-walks" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="why-does-this-work">
<h3>Why does this work?<a class="headerlink" href="#why-does-this-work" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="proposal-distribution">
<h3>Proposal distribution<a class="headerlink" href="#proposal-distribution" title="Permalink to this headline">¶</a></h3>
<p>An easy to sample distribution such as a Gaussian distribution <span class="math notranslate nohighlight">\(q(x)\)</span> such that</p>
<p><span class="math notranslate nohighlight">\(q(x_{i+1} | x_{i}) \approx N(\mu, \sigma)\)</span></p>
</div>
<div class="section" id="foundation-of-bayesian-inference">
<h3>Foundation of Bayesian Inference<a class="headerlink" href="#foundation-of-bayesian-inference" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Obtain the data and inspect it for a high-level understanding of the distribution of the data and the outliers</p></li>
<li><p>Define a reasonable prior for the data based on (1) and your understanding of the problem</p></li>
<li><p>Define a likelihood distribution for the data and obtain the likelihood of the data given this likelihood distribution</p></li>
<li><p>Obtain the posterior distribution using (2) and (3) by applying the Bayes Theorem</p></li>
</ol>
</div>
</div>
<div class="section" id="the-metropolis-algorithm">
<h2>The Metropolis Algorithm<a class="headerlink" href="#the-metropolis-algorithm" title="Permalink to this headline">¶</a></h2>
<p>We start off by modeling a discrete number of events using a Poisson distribution shown below.</p>
<p><span class="math notranslate nohighlight">\(f(x) = e^{-\mu} \mu^x / x!\)</span></p>
<p>The mean rate is represented by μ and x is positive integer that represents the number of events that can happen. If you recall from the discussion of the binomial distribution, that can also be used to model the probability of the number of successes out of ‘n’ trials. The Poisson distribution is a special case of this binomial distribution and is used when the trials far exceed the number of successes.</p>
<p>If our observed data has a Poisson likelihood distribution, using a Gamma prior for <span class="math notranslate nohighlight">\(\mu\)</span> results in a Gamma posterior distribution.</p>
<div class="section" id="outline-of-the-metropolis-algorithm">
<h3>Outline of the Metropolis algorithm<a class="headerlink" href="#outline-of-the-metropolis-algorithm" title="Permalink to this headline">¶</a></h3>
<p><em>What do we want to compute?</em></p>
<p>To estimate a distribution of a parameter <span class="math notranslate nohighlight">\(\mu\)</span></p>
<p><em>What do we have available?</em></p>
<p>Observed data</p>
<p><em>How do we do it?</em></p>
<ol class="simple">
<li><p>Start with a parameter sample <span class="math notranslate nohighlight">\(\mu_{current}\)</span> that is drawn from a distribution</p></li>
<li><p>Draw a second parameter sample <span class="math notranslate nohighlight">\(\mu_{proposed}\)</span> from a proposal distribution</p></li>
<li><p>Compute the likelihood of the data for both the parameters</p></li>
<li><p>Compute the prior probability density of both the parameters</p></li>
<li><p>Compute the posterior probability density of both parameters by multiplying the prior and the likelihood from (3) and (4)</p></li>
<li><p>Select one from the posterior probability density computed above using a rule and save the selected one as <span class="math notranslate nohighlight">\(\mu_{current}\)</span></p></li>
<li><p>Repeat steps (2) to (7) till a large number of parameters have been drawn (usually around 5000, but this really depends on the problem)</p></li>
<li><p>Compute the distribution of the parameter <span class="math notranslate nohighlight">\(\mu\)</span> by plotting a histogram of the saved sampled parameter <span class="math notranslate nohighlight">\(\mu_{current}\)</span> in step (6)</p></li>
</ol>
</div>
<div class="section" id="the-details">
<h3>The details<a class="headerlink" href="#the-details" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>Propose a single plausible value for our parameter <span class="math notranslate nohighlight">\(\mu\)</span>. This is <span class="math notranslate nohighlight">\(\mu_{current}\)</span> from the previous section. This is also called the current value. Let us assume that this is 7.5 for now.</p></li>
<li><p>Compute the prior probability density of getting 7.5. We stated earlier in our example that we have a Gamma prior distribution for our parameter <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><span class="math notranslate nohighlight">\(Gamma(x=7.5, \alpha, \beta) = \beta^{\alpha} x^{\alpha - 1} e^{-\beta x} / \gamma(\alpha) = \beta^{\alpha} 7.5^{\alpha - 1} e^{-\beta 7.5} / \gamma(\alpha)\)</span></p>
</li>
<li><p>Compute the likelihood of the data given the parameter value of 7.5. The likelihood distribution was a Poisson distribution in our example</p>
<p><span class="math notranslate nohighlight">\(Poisson(x, mu=7.5) = e^{-\mu} \mu^x / x! = e^{-7.5} 7.5^x / x!\)</span></p>
</li>
<li><p>Compute the posterior density from (2) and (3), we skip the denominator here since we are only going to make comparisons and the denominator is a constant.</p>
<p>Posterior density <span class="math notranslate nohighlight">\(\propto\)</span> Prior <span class="math notranslate nohighlight">\(\cdot\)</span> likelihood</p>
</li>
<li><p>Propose a second value for <span class="math notranslate nohighlight">\(\mu\)</span>, called <span class="math notranslate nohighlight">\(\mu_{proposed}\)</span>, which is drawn from a distribution called a proposal distribution centered on <span class="math notranslate nohighlight">\(mu_{current}\)</span>. This value is called the proposed value. For the Metropolis algorithm, it has to be a symmetrical distribution. We will use a normal distribution for this example and set the mean of this proposal distribution to be the current value of <span class="math notranslate nohighlight">\(\mu\)</span>. The standard deviation is a hyperparameter called the tuning parameter. Let us assume that we draw a value of 8.5.</p></li>
<li><p>Compute the prior, likelihood and the posterior for this proposed value of <span class="math notranslate nohighlight">\(\mu\)</span> as we did in step (2), (3) and (4).</p></li>
<li><p>Select one value from the current and the proposed value with the following two steps (this step is where the Metropolis algorithm differs from the Metropolis-Hastings algorithm)</p>
<p>a. Compute the probability of moving to the proposed value as
<span class="math notranslate nohighlight">\(p_{move} = min( \dfrac{P(\mu_{proposed} | data)}{P(\mu_{current} | data)}, 1)\)</span></p>
<p>Here <span class="math notranslate nohighlight">\(p_{move}\)</span> is the minimum of the values given by the ratio of the probabilities and the number 1. This caps the probability <span class="math notranslate nohighlight">\(p_{move}\)</span> at 1 if the ratio happens to be greater than 1. <span class="math notranslate nohighlight">\(P_{move}\)</span> is also referred to as the transition kernel.</p>
<p>b. Draw a sample from a uniform distribution U(0,1). If <span class="math notranslate nohighlight">\(p_{move}\)</span> from (a) above is greater than this number drawn from the uniform distribution, we accept the proposed value <span class="math notranslate nohighlight">\(\mu_{proposed}\)</span>. What this means is that if the posterior density of the proposed parameter value is greater than the posterior density of the current parameter value, then we move to the proposed value otherwise we probabilistically accept the proposed value based on the value of <span class="math notranslate nohighlight">\(p_{move}\)</span> and the randomly drawn value from the uniform distribution.</p>
</li>
<li><p>If we moved to the proposed value, save the current value to an array and then update the current value with the proposed value. In the next iteration, the current value <span class="math notranslate nohighlight">\(\mu^{i+1}_{current}\)</span> will be this accepted proposed value <span class="math notranslate nohighlight">\(\mu^{i}_{proposed}\)</span>.</p></li>
<li><p>Repeat steps (2) to (8) thousands of times and plot the histogram of the accepted values, i.e. the array of current values <span class="math notranslate nohighlight">\(\mu_{current}\)</span>.</p></li>
</ol>
</div>
<div class="section" id="traceplot">
<h3>Traceplot<a class="headerlink" href="#traceplot" title="Permalink to this headline">¶</a></h3>
<p>The sequence of accepted values from the proposed values that is plotted over each draw. If a proposed value was not accepted, you see the same value repeated again. If you notice a straight line, this is an indication that several proposed values are being rejected. This is a sign that something is askew with the distribution or sampling process.</p>
</div>
<div class="section" id="building-the-inferred-distribution">
<h3>Building the Inferred Distribution<a class="headerlink" href="#building-the-inferred-distribution" title="Permalink to this headline">¶</a></h3>
<p>Use the current values that we obtain at each step and build a frequency distribution (histogram) from it.</p>
</div>
<div class="section" id="representing-the-inferred-distribution">
<h3>Representing the Inferred Distribution<a class="headerlink" href="#representing-the-inferred-distribution" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Compute the mean values of the saved parameters</p></li>
<li><p>Compute the standard deviation and variance of the saved parameters</p></li>
<li><p>Compute the minimum and maximum values of the saved parameters</p></li>
<li><p>Compute the quantiles of the saved parameters</p></li>
<li><p>If required, express it as the parameters of a canonical distribution if it is known that the inferred distribution will be of a certain form.</p></li>
</ul>
</div>
<div class="section" id="notes-about-the-metropolis-algorithm">
<h3>Notes about the Metropolis algorithm<a class="headerlink" href="#notes-about-the-metropolis-algorithm" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The proposal distribution has to be symmetric, this condition is relaxed in the Metropolis-Hastings algorithm. A normal distribution is commonly used as a proposal distribution in the Metropolis algorithm.</p></li>
<li><p>The choice of a prior distribution influences the performance of this algorithm.</p></li>
<li><p>Tuning - A hyperparameter, i.e. the standard deviation is essential to tune this proposal distribution. This needs to be tuned such that the acceptance probability is a certain value. This is referred to as the tuning parameter.</p></li>
</ul>
</div>
<div class="section" id="python-code-for-walkthrough-of-algorithm">
<h3>Python Code for walkthrough of algorithm<a class="headerlink" href="#python-code-for-walkthrough-of-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Using the example above and the shark attack problem, run a simulation for 1000 iterations</p>
</div>
<div class="section" id="summarize-the-above-distribution-mean-variance-minimum-and-maximum-quartiles">
<h3>Summarize the above distribution - Mean, Variance, Minimum and Maximum, Quartiles<a class="headerlink" href="#summarize-the-above-distribution-mean-variance-minimum-and-maximum-quartiles" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="the-metropolis-hastings-algorithm">
<h2>The Metropolis-Hastings Algorithm<a class="headerlink" href="#the-metropolis-hastings-algorithm" title="Permalink to this headline">¶</a></h2>
<p>One of the limitations of the Metropolis algorithm was the requirement of a symmetric proposal distribution. The Metropolis-Hastings relaxes this requirement by providing a correction term if a non-symmetric proposal distribution is used. This correction is applied to <span class="math notranslate nohighlight">\(p_{move}\)</span> and is of the form</p>
<p><span class="math notranslate nohighlight">\(p_{move} = min( \dfrac{P(\mu_{proposed} | data) \cdot g(\mu_{current} | \mu_{proposed})}{P(\mu_{current} | data) \cdot g(\mu_{proposed} | \mu_{current})}, 1)\)</span></p>
<p>where the correction term is</p>
<p><span class="math notranslate nohighlight">\(\dfrac{g(\mu_{current} | \mu_{proposed})}{g(\mu_{proposed} | \mu_{current})}\)</span></p>
<p>The term <span class="math notranslate nohighlight">\(g(\mu_{current} | \mu_{proposed})\)</span> is the probability density of drawing <span class="math notranslate nohighlight">\(\mu_{current}\)</span> from a normal distribution centered around <span class="math notranslate nohighlight">\(\mu_{proposed}\)</span>. The standard deviation for this normal distribution is the tuning parameter. For a symmetric proposal distribution such as a normal distribution the correction term would be 1 since the probability density of drawing <span class="math notranslate nohighlight">\(\mu_{current}\)</span> from a Gaussian centered at <span class="math notranslate nohighlight">\(\mu_{proposal}\)</span> only depends on the distance between <span class="math notranslate nohighlight">\(\mu_{current}\)</span> and <span class="math notranslate nohighlight">\(\mu_{proposal}\)</span> (standard deviation is a hyperparameter that is fixed). Similarly, the probability density of drawing <span class="math notranslate nohighlight">\(\mu_{proposal}\)</span> from a Gaussian centered around <span class="math notranslate nohighlight">\(\mu_{current}\)</span> depends only on the distance between these two values, which is the same as before. Hence the numerator and the denominator are the same which results in the correction factor being 1.</p>
</div>
<div class="section" id="hamiltonian-monte-carlo-also-called-hybrid-monte-carlo">
<h2>Hamiltonian Monte Carlo (also called Hybrid Monte Carlo)<a class="headerlink" href="#hamiltonian-monte-carlo-also-called-hybrid-monte-carlo" title="Permalink to this headline">¶</a></h2>
<p>Based on the solution of differential equations known as Hamilton’s equations. These differential equations depend on the probability distributions we are trying to learn. We navigate these distributions by moving around them in a trajectory using steps that are defined by a position and momentum at that position. Navigating these trajectories can be a very expensive process and the goal is to minimize this computational process.</p>
<p>HMC is based on the notion of conservation of energy. When the sampler trajectory is far away from the probability mass center, it has high potential energy but low kinetic energy and when it is closer to the center of the probability mass will have high kinetic energy but low potential energy.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="notebooks.html" title="previous page">Bayesian vs Frequentist statistics</a>
    <a class='right-next' id="next-link" href="PyMC3.html" title="next page">Introduction to PyMC3</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Srijith Rajamohan, Ph.D.<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>